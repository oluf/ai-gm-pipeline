import os
import subprocess
import logging
from fastapi import FastAPI
import chromadb
from sentence_transformers import SentenceTransformer

# Define path and filename constants
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CHROMADB_PATH = os.path.join(BASE_DIR, "../data/rpg_sources_db")
DB_COLLECTION = "rpg_sources"

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

app = FastAPI()

#   Configure the ChromaDB client and collection
client = chromadb.PersistentClient(path=CHROMADB_PATH)
collection = client.get_or_create_collection(DB_COLLECTION)

#   Load the SentenceTransformer model
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Define llama-cpp constants
LLAMA_BINARY = "/home/oluf/bin/llama.cpp/build/bin/llama-run"
MODEL_FILE = "/home/oluf/projects/ai-gm-pipeline/models/mythomax-13B.Q4_K_M.gguf"
GPU_LAYERS = "35"
TEMPERATURE = "0.7"


def query_llama(query, context):
    """
        Send a query along with the retrieved rules to Llama.cpp for processing.

        Args:
            query (str): The search query.
            context (str): The context retrieved from the rulebook.

        Returns:
            str: The response generated by Llama.cpp based on the query and context.
    """
    full_prompt = f"Using the following rulebook context:\n\n{context}\n\nAnswer the player's question: {query}"
    command = [
        LLAMA_BINARY,
        "--ngl", GPU_LAYERS,
        "--temp", TEMPERATURE,
        MODEL_FILE,
        full_prompt
    ]

    logging.info("üöÄ Running Llama.cpp with command: %s", " ".join(command))

    try:
        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, error = process.communicate()

        if error:
            logging.error("‚ùå Llama.cpp Error: %s", error.decode())

        result = output.decode().strip()
        logging.info("üìù Llama Output: %s", result)
        return result
    except Exception as e:
        logging.exception("Exception occurred while running Llama.cpp")
        return "Error occurred during LLM processing."

@app.get("/")
def read_root():
    """
        Root endpoint to check if FastAPI is running.

        Returns:
            dict: A message indicating that FastAPI is running.
    """
    return {"message": "FastAPI is running!"}


@app.get("/search")
def search(query: str):
    """
       Endpoint to retrieve relevant RPG rules based on a query.

       Args:
           query (str): The search query.

       Returns:
           dict: The search results containing the most relevant RPG rules.
    """
    query_embedding = embedding_model.encode(query)  # Convert query to embedding
    search_results = collection.query(query_embeddings=[query_embedding], n_results=3)
    return {"response": search_results['documents'][0]}


@app.get("/ai-search")
def ai_search(query: str):
    """
    Endpoint to retrieve rules and enhance them with AI.

    Args:
        query (str): The search query.

    Returns:
        dict: The AI-enhanced response based on the retrieved rules.
    """
    query_embedding = embedding_model.encode(query)  # Convert query to embedding
    search_results = collection.query(query_embeddings=[query_embedding], n_results=3)
    # Join all retrieved document chunks into a single context string
    retrieved_context = "\n".join(search_results['documents'][0])

    ai_response = query_llama(query, retrieved_context)
    return {"response": ai_response}